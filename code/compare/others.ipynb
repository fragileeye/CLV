{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, cohen_kappa_score, confusion_matrix \n",
    "from spot import *\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess:\n",
    "    INVALID_ITEM = -1\n",
    "    def __init__(self, data_file, data_size, config):\n",
    "        self.file = data_file\n",
    "        self.data_size = data_size\n",
    "        self.fake_eps = config['fake_eps']\n",
    "        self.min_delay = config['min_delay']\n",
    "        self.max_delay = config['max_delay']\n",
    "        self.max_bw = np.log(config['max_bw']*1024*1024)\n",
    "        self.cong_rate = config['cong_rate']\n",
    "        self.stat_size = config['stat_size']\n",
    "        self.avg_weight = config['avg_weight']\n",
    "        self.link_delay_map = {}\n",
    "        self.path_delay_map = {}\n",
    "        self.link_loads_map = {}\n",
    "        self.valid_delay_map = {}\n",
    "        self.valid_loads_map = {}\n",
    "        self.delay_limit_map = {}\n",
    "        self.quality_map = {}\n",
    "        self.dataset = []\n",
    "        self.target_dpids = self._get_target_dpids()\n",
    "    \n",
    "    def preprocess(self):\n",
    "        with open(self.file, 'r+') as fp:\n",
    "            for _ in range(self.data_size):\n",
    "                line = fp.readline()\n",
    "                if not line:\n",
    "                    break \n",
    "                self._get_link_delay(line)\n",
    "                self._get_data_item(line)\n",
    "                self._get_link_quality(line)\n",
    "    \n",
    "    def _get_target_dpids(self):\n",
    "        target_dpids = set()\n",
    "        for ep in self.fake_eps:\n",
    "            dpid = ep[0]\n",
    "            target_dpids.add(dpid)\n",
    "        return target_dpids\n",
    "    \n",
    "    def _get_link_delay(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        loads = float(res_list[-1])\n",
    "        path_delay = float(res_list[-2])\n",
    "        cs_delay = float(res_list[-3])\n",
    "        sc_delay = float(res_list[-4])\n",
    "        link_delay = path_delay - (cs_delay + sc_delay)/2\n",
    "        if link_delay < self.min_delay:\n",
    "            link_delay = self.min_delay\n",
    "        link_delays = self.link_delay_map.setdefault(link, [])\n",
    "        link_delays.append(link_delay)\n",
    "        link_loads = self.link_loads_map.setdefault(link, [])\n",
    "        link_loads.append(loads)\n",
    "        path_delays = self.path_delay_map.setdefault(link, [])\n",
    "        path_delays.append(path_delay)\n",
    "    \n",
    "    def _get_valid_delay(self, link, delay):\n",
    "        valid_list = self.valid_delay_map.setdefault(link, [self.max_delay])\n",
    "        if delay == DataProcess.INVALID_ITEM:\n",
    "            if len(valid_list) < self.stat_size:\n",
    "                item_list = self.link_delay_map[link]\n",
    "                s_loc = -min(len(item_list), self.stat_size)\n",
    "                valid_delay = np.median(item_list[s_loc:])\n",
    "            else:\n",
    "                valid_delay = np.median(valid_list)\n",
    "        else:\n",
    "            if len(valid_list) >= self.stat_size:\n",
    "                valid_list.pop(0)\n",
    "            valid_list.append(delay)\n",
    "            if len(valid_list) < len(self.avg_weight):\n",
    "                valid_delay = sum(valid_list)/len(valid_list)\n",
    "            else:\n",
    "                s_loc = -len(self.avg_weight)\n",
    "                valid_delay = sum([self.avg_weight[i]*d \\\n",
    "                    for i, d in enumerate(valid_list[s_loc:])])\n",
    "        return valid_delay\n",
    "        \n",
    "    def _get_valid_loads(self, link, loads):\n",
    "        valid_list = self.valid_loads_map.setdefault(link, [0])\n",
    "        if loads == DataProcess.INVALID_ITEM:\n",
    "            valid_loads = 0\n",
    "        else:\n",
    "            if len(valid_list) >= self.stat_size:\n",
    "                valid_list.pop(0)\n",
    "            valid_list.append(loads)\n",
    "            if len(valid_list) < len(self.avg_weight):\n",
    "                valid_loads = sum(valid_list)/len(valid_list)\n",
    "            else:\n",
    "                s_loc = -len(self.avg_weight)\n",
    "                valid_loads = sum([self.avg_weight[i]*d \\\n",
    "                    for i, d in enumerate(valid_list[s_loc:])])\n",
    "        return valid_loads\n",
    "        \n",
    "    def _get_link_quality(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        delay = float(res_list[-2])\n",
    "        loads = float(res_list[-1])\n",
    "        \n",
    "        # The key idea of this tricks locates at that, the delay beyond a so called 'max_delay' should\n",
    "        # make no sense to indicate the link quality, as well as the reliability of the links.\n",
    "        # Statistic of the median delay of the links of the target dpid, so as to obtain the valid delay\n",
    "        # which is used to indicated the reliability of the links.\n",
    "        \n",
    "        if delay > self.max_delay:\n",
    "            delay = self._get_valid_delay(link, \n",
    "                                         DataProcess.INVALID_ITEM)\n",
    "            loads = self._get_valid_loads(link,\n",
    "                                         DataProcess.INVALID_ITEM)\n",
    "        else:\n",
    "            delay = self._get_valid_delay(link, delay)\n",
    "            loads = self._get_valid_loads(link, loads)\n",
    "            \n",
    "        tq = 1 - np.tanh(delay/self.max_delay)\n",
    "        lq = 1 / (1 + np.exp(-(loads-self.max_bw*self.cong_rate)))\n",
    "        \n",
    "        # The degree of indicating link quality mainly depends on the delay quality, e.g., \n",
    "        # Link A: high loads, low delay\n",
    "        # Link B: low loads, low delay\n",
    "        # Link C: high loads, high delay\n",
    "        # Link D: low loads, high delay \n",
    "        # we hat A > B > C > D\n",
    "        # Eq. Q = (Qt + np.exp(Qt-max_Qt) * Ql) / 2, where max_Qt = 1-np.tanh(0) = 1\n",
    "        quality = tq * (1 + np.exp(tq - 1) * lq) / 2 \n",
    "        quality_list = self.quality_map.setdefault(link, [])\n",
    "        quality_list.append(quality)\n",
    "    \n",
    "    def _get_data_item(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        dpid = link.split(':')[0]\n",
    "        if not dpid in self.target_dpids:\n",
    "            return \n",
    "        path_delay = float(res_list[-2])\n",
    "        cs_delay = float(res_list[-3])\n",
    "        sc_delay = float(res_list[-4])\n",
    "        delay = path_delay - (cs_delay + sc_delay)/2\n",
    "        if delay < self.min_delay:\n",
    "            delay = self.min_delay\n",
    "        if any([x in link for x in self.fake_eps]):\n",
    "            is_forged_link = True \n",
    "        else:\n",
    "            is_forged_link = False \n",
    "        data_item = [cs_delay, sc_delay, delay, is_forged_link]\n",
    "        self.dataset.append(data_item)\n",
    "    \n",
    "    def query_quality_map(self):\n",
    "        return self.quality_map \n",
    "    \n",
    "    def query_delay_map(self):\n",
    "        return self.link_delay_map\n",
    "    \n",
    "    def query_delay_map2(self):\n",
    "        return self.path_delay_map\n",
    "    \n",
    "    def query_dataset(self):\n",
    "        return np.array(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopoGuardPlus:\n",
    "    def __init__(self, delay_map, start_range, test_range, fake_eps):\n",
    "        self.delay_map = delay_map\n",
    "        self.start_range = start_range \n",
    "        self.test_range = test_range\n",
    "        self.target_dpids = self._get_target_dpids(fake_eps)\n",
    "        self.fake_links = self._get_fake_links(fake_eps)\n",
    "        self.link_delay_queue = self._prepare()\n",
    "    \n",
    "    def _get_target_dpids(self, fake_eps):\n",
    "        target_dpids = set()\n",
    "        for ep in fake_eps:\n",
    "            dpid = ep[0]\n",
    "            target_dpids.add(dpid)\n",
    "        return target_dpids\n",
    "    \n",
    "    def _get_fake_links(self, fake_eps):\n",
    "        fake_links = []\n",
    "        for link in self.delay_map.keys():\n",
    "            for ep in fake_eps:\n",
    "                if ep in link:\n",
    "                    fake_links.append(link)\n",
    "                    break \n",
    "        return fake_links \n",
    "    \n",
    "    def _prepare(self):\n",
    "        link_delay_queue = []\n",
    "        for i in self.start_range:\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid in self.target_dpids and \\\n",
    "                    link not in self.fake_links:\n",
    "                    link_delay_queue.append(delay_list[i])\n",
    "        return link_delay_queue\n",
    "    \n",
    "    def fit(self):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i in self.test_range:\n",
    "            Q1 = np.quantile(self.link_delay_queue, 0.25)\n",
    "            Q3 = np.quantile(self.link_delay_queue, 0.75)\n",
    "            thr = Q3 + 3 * (Q3 - Q1)\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid not in self.target_dpids:\n",
    "                    continue\n",
    "                curr_delay = delay_list[i]\n",
    "                self.link_delay_queue.append(curr_delay)\n",
    "                pred_flag = 1 if curr_delay > thr else False \n",
    "                true_flag = 1 if link in self.fake_links else False\n",
    "                y_pred.append(pred_flag) \n",
    "                y_true.append(true_flag)\n",
    "        return y_pred, y_true             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrustTopo:\n",
    "    def __init__(self, delay_map, start_range, test_range, fake_eps):\n",
    "        self.delay_map = delay_map\n",
    "        self.start_range = start_range \n",
    "        self.test_range = test_range\n",
    "        self.target_dpids = self._get_target_dpids(fake_eps)\n",
    "        self.fake_links = self._get_fake_links(fake_eps)\n",
    "        self.link_delay_queue = self._prepare()\n",
    "        self.C = 0.5\n",
    "\n",
    "    def _get_target_dpids(self, fake_eps):\n",
    "        target_dpids = set()\n",
    "        for ep in fake_eps:\n",
    "            dpid = ep[0]\n",
    "            target_dpids.add(dpid)\n",
    "        return target_dpids\n",
    "    \n",
    "    def _get_fake_links(self, fake_eps):\n",
    "        fake_links = []\n",
    "        for link in self.delay_map.keys():\n",
    "            for ep in fake_eps:\n",
    "                if ep in link:\n",
    "                    fake_links.append(link)\n",
    "                    break \n",
    "        return fake_links \n",
    "    \n",
    "    def _prepare(self):\n",
    "        link_delay_queue = []\n",
    "        for i in self.start_range:\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid in self.target_dpids and \\\n",
    "                    link not in self.fake_links:\n",
    "                    link_delay_queue.append(delay_list[i])\n",
    "        # sort\n",
    "        link_delay_queue.sort()\n",
    "        # strip the ends \n",
    "        link_delay_queue.pop(0)\n",
    "        link_delay_queue.pop(-1)\n",
    "        return link_delay_queue\n",
    "\n",
    "    def train(self, delay):\n",
    "        tmp_queue = np.array(self.link_delay_queue)\n",
    "        med = np.median(tmp_queue)\n",
    "        prop = np.sum(tmp_queue > med) / len(tmp_queue)\n",
    "        delta = max(delay-med, 0)\n",
    "        thr = med + prop * delta + self.C\n",
    "        return thr\n",
    "    \n",
    "    def fit(self):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        thr = self.train(0)\n",
    "        for i in self.test_range:\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid not in self.target_dpids:\n",
    "                    continue\n",
    "                curr_delay = delay_list[i]\n",
    "                pred_flag = True if curr_delay > thr else False \n",
    "                true_flag = True if link in self.fake_links else False\n",
    "                y_pred.append(pred_flag) \n",
    "                y_true.append(true_flag)\n",
    "                # update \n",
    "                if not pred_flag:\n",
    "                    self.link_delay_queue.append(curr_delay)\n",
    "                    thr = self.train(curr_delay)\n",
    "                #print('thr: {}, current delay: {}'.format(thr, curr_delay))\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同于其它方法，LinkGuard本质上是静态的，所以它的阈值是不更新的。\n",
    "class LinkGuard:\n",
    "    def __init__(self, delay_map, train_range, test_range, fake_eps):\n",
    "        self.delay_map = delay_map\n",
    "        self.train_range = train_range\n",
    "        self.test_range = test_range\n",
    "        self.target_dpids = self._get_target_dpids(fake_eps)\n",
    "        self.fake_links = self._get_fake_links(fake_eps)\n",
    "    \n",
    "    def _get_target_dpids(self, fake_eps):\n",
    "        target_dpids = set()\n",
    "        for ep in fake_eps:\n",
    "            dpid = ep[0]\n",
    "            target_dpids.add(dpid)\n",
    "        return target_dpids\n",
    "    \n",
    "    def _get_fake_links(self, fake_eps):\n",
    "        fake_links = []\n",
    "        for link in self.delay_map.keys():\n",
    "            for ep in fake_eps:\n",
    "                if ep in link:\n",
    "                    fake_links.append(link)\n",
    "                    break \n",
    "        return fake_links \n",
    "    \n",
    "    def train(self):\n",
    "        link_delay_queue = []\n",
    "        for i in self.train_range:\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid in self.target_dpids and \\\n",
    "                    link not in self.fake_links:\n",
    "                    link_delay_queue.append(delay_list[i])\n",
    "        Q1 = np.quantile(link_delay_queue, 0.25)\n",
    "        Q3 = np.quantile(link_delay_queue, 0.75)\n",
    "        thr = Q3 + 3 * (Q3 - Q1)\n",
    "        return thr\n",
    "    \n",
    "    def fit(self):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        thr = self.train()\n",
    "        for i in self.test_range:\n",
    "            for link, delay_list in self.delay_map.items():\n",
    "                dpid = link.split(':')[0]\n",
    "                if dpid not in self.target_dpids:\n",
    "                    continue \n",
    "                curr_delay = delay_list[i]\n",
    "                pred_flag = True if curr_delay > thr else False \n",
    "                true_flag = True if link in self.fake_links else False \n",
    "                y_pred.append(pred_flag)\n",
    "                y_true.append(true_flag)\n",
    "        return y_pred, y_true \n",
    "    # def fit(self):\n",
    "    #     y_pred = []\n",
    "    #     y_true = []\n",
    "    #     thr = self.train()\n",
    "    #     # we don't set it to 10 as the original paper, because the performance is much worse than TTL=1\n",
    "    #     TTL = 1 \n",
    "    #     measure_result = {}\n",
    "    #     pred_result = {}\n",
    "    #     for i in self.test_range:\n",
    "    #         for link, delay_list in self.delay_map.items():\n",
    "    #             dpid = link.split(':')[0]\n",
    "    #             if dpid not in self.target_dpids:\n",
    "    #                 continue\n",
    "    #             measure_result.setdefault(link, [])\n",
    "    #             pred_result.setdefault(link, [])\n",
    "    #             curr_delay = delay_list[i]\n",
    "    #             measure_result[link].append(curr_delay)\n",
    "    #             if len(measure_result[link]) % TTL == 0:\n",
    "    #                 measure_dealy = np.median(measure_result[link])\n",
    "    #                 measure_result[link].clear()\n",
    "    #                 pred_flag = True if measure_dealy > thr else False\n",
    "    #                 pred_result[link].extend([pred_flag] * TTL) \n",
    "    #             true_flag = True if link in self.fake_links else False\n",
    "    #             y_true.append(true_flag)\n",
    "    #     # In python 3.9, the keys are fix-ordered in dict.\n",
    "    #     batch_size = len(self.test_range)\n",
    "    #     for i in range(batch_size):\n",
    "    #         for v in pred_result.values():\n",
    "    #             y_pred.append(v[i])\n",
    "    #     assert(len(y_pred) == len(y_true)) \n",
    "    #     return y_pred, y_true      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_metrics(model_name, y_true, y_pred):   \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cnf_matrix.ravel()\n",
    "    \n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    \n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "    \n",
    "    Pr = precision_score(y_true, y_pred)\n",
    "    F1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Error, the 2nd param of roc_auc_score is not y_pred\n",
    "    # but y_score, which is the probability of y_pred \n",
    "    AUC = roc_auc_score(y_true, y_pred)\n",
    "    print(f'''\n",
    "          name : {model_name},\n",
    "          TPR: {TPR},\n",
    "          FPR: {FPR},\n",
    "          Precision: {Pr},\n",
    "          f1_score: {F1},\n",
    "          auc_score: {AUC}\n",
    "          ''')\n",
    "    return TPR, FPR, Pr, F1, AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model_predict(model, model_name, dataset):\n",
    "    dataset = np.array(dataset)\n",
    "    X, y = dataset[:,:-1], dataset[:,-1]\n",
    "    multi_step_model = ['lr', 'rf', 'svc', 'mlp']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.334, random_state=0)\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    if model_name in multi_step_model:\n",
    "        y_pred[y_pred > 0.5] = 1\n",
    "        y_pred[y_pred <= 0.5] = 0\n",
    "    return y_pred, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_result(model, model_name, X, y, param_grid):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X ,y)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic test\n",
    "num_switch = 5\n",
    "\n",
    "data_files = [\n",
    "    r'../dataset/{}switches/pcap_link_0.5_no_load_no_attack.txt'.format(num_switch),\n",
    "    r'../dataset/{}switches/pcap_link_0.5_no_load_attack.txt'.format(num_switch),\n",
    "    r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch),]\n",
    "\n",
    "data_size = 14*3000\n",
    "\n",
    "processors = [\n",
    "    DataProcess(data_file, data_size, ds_config) for data_file in data_files]\n",
    "\n",
    "for p in processors:\n",
    "    p.preprocess()\n",
    "\n",
    "delays = [p.query_delay_map() for p in processors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log TopoGuard+ result\n",
    "start_range = range(0, 2000)\n",
    "test_range = range(2000, 3000)\n",
    "\n",
    "topoguard_plus = TopoGuardPlus(delays[0], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = topoguard_plus.fit()\n",
    "show_model_metrics('TopoGuardPlus', y_true, y_pred)\n",
    "\n",
    "topoguard_plus = TopoGuardPlus(delays[1], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = topoguard_plus.fit()\n",
    "show_model_metrics('TopoGuardPlus', y_true, y_pred)\n",
    "\n",
    "topoguard_plus = TopoGuardPlus(delays[2], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = topoguard_plus.fit()\n",
    "show_model_metrics('TopoGuardPlus', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_range = range(0, 2000)\n",
    "test_range = range(2000, 3000)\n",
    "\n",
    "trust_topo = TrustTopo(delays[0], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = trust_topo.fit()\n",
    "show_model_metrics('TrustTopo', y_true, y_pred)\n",
    "\n",
    "trust_topo = TrustTopo(delays[1], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = trust_topo.fit()\n",
    "show_model_metrics('TrustTopo', y_true, y_pred)\n",
    "\n",
    "\n",
    "trust_topo = TrustTopo(delays[2], start_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = trust_topo.fit()\n",
    "show_model_metrics('TrustTopo', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = range(0, 2000)\n",
    "test_range = range(2000, 3000)\n",
    "\n",
    "link_guard = LinkGuard(delays[0], train_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = link_guard.fit()\n",
    "show_model_metrics('LinkGuard', y_true, y_pred)\n",
    "\n",
    "link_guard = LinkGuard(delays[1], train_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = link_guard.fit()\n",
    "show_model_metrics('LinkGuard', y_true, y_pred)\n",
    "\n",
    "\n",
    "link_guard = LinkGuard(delays[2], train_range, test_range, ['2:1', '4:1'])\n",
    "y_pred, y_true = link_guard.fit()\n",
    "show_model_metrics('LinkGuard', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [processor.query_dataset() for processor in processors]\n",
    "\n",
    "lr = LinearRegression()\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "svc = SVC(random_state=0)\n",
    "knn = KNeighborsClassifier()\n",
    "mlp = MLPClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "\n",
    "y_pred, y_true = query_model_predict(lr, 'lr', datasets[0])\n",
    "show_model_metrics('lr', y_true, y_pred)\n",
    "\n",
    "y_pred, y_true = query_model_predict(lr, 'lr', datasets[1])\n",
    "show_model_metrics('lr', y_true, y_pred)\n",
    "\n",
    "y_pred, y_true = query_model_predict(lr, 'lr', datasets[2])\n",
    "show_model_metrics('lr', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = query_model_predict(dt, 'dt', datasets[0])\n",
    "show_model_metrics('dt', y_true, y_pred)\n",
    "\n",
    "y_pred, y_true = query_model_predict(dt, 'dt', datasets[1])\n",
    "show_model_metrics('dt', y_true, y_pred)\n",
    "\n",
    "y_pred, y_true = query_model_predict(dt, 'dt', datasets[2])\n",
    "show_model_metrics('dt', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "# {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 16} for dataset 0\n",
    "# {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 20} for dataset 1\n",
    "# {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 20} for dataset 2\n",
    "# dataset = datasets[2]\n",
    "# param_grid = {\n",
    "#     'n_estimators': [i for i in range(1, 21)],\n",
    "#     'max_depth': [i for i in range(1, 11)]\n",
    "# }\n",
    "\n",
    "# X, y = dataset[:,:-1], dataset[:,-1]\n",
    "# search_result(rf, 'rf', X, y, param_grid)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=16)\n",
    "y_pred, y_true = query_model_predict(rf, 'rf', datasets[0])\n",
    "show_model_metrics('rf', y_true, y_pred)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=20)\n",
    "y_pred, y_true = query_model_predict(rf, 'rf', datasets[1])\n",
    "show_model_metrics('rf', y_true, y_pred)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=20)\n",
    "y_pred, y_true = query_model_predict(rf, 'rf', datasets[2])\n",
    "show_model_metrics('rf', y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "# {'C': 5000} for all dataset\n",
    "# dataset = datasets[2]\n",
    "# print(len(dataset))\n",
    "# param_grid = {\n",
    "#     'C': [i for i in range(1000, 6000, 1000)],\n",
    "# }\n",
    "# X, y = dataset[:,:-1], dataset[:,-1]\n",
    "# search_result(svc, 'svc', X, y, param_grid)\n",
    "\n",
    "svc = SVC(random_state=0, C=5000)\n",
    "y_pred, y_true = query_model_predict(svc, 'svc', datasets[0])\n",
    "show_model_metrics('svc', y_true, y_pred)\n",
    "\n",
    "svc = SVC(random_state=0, C=5000)\n",
    "y_pred, y_true = query_model_predict(svc, 'svc', datasets[1])\n",
    "show_model_metrics('svc', y_true, y_pred)\n",
    "\n",
    "svc = SVC(random_state=0, C=5000)\n",
    "y_pred, y_true = query_model_predict(svc, 'svc', datasets[2])\n",
    "show_model_metrics('svc', y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "# {'n_neighbors': 20} for all dataset\n",
    "\n",
    "#dataset = datasets[2]\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [i for i in range(1, 21)],\n",
    "# }\n",
    "\n",
    "# X, y = dataset[:,:-1], dataset[:,-1]\n",
    "# search_result(knn, 'knn', X, y, param_grid)\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=20)\n",
    "y_pred, y_true = query_model_predict(knn1, 'knn', datasets[0])\n",
    "show_model_metrics('knn', y_true, y_pred)\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=20)\n",
    "y_pred, y_true = query_model_predict(knn2, 'knn', datasets[1])\n",
    "show_model_metrics('knn', y_true, y_pred)\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=20)\n",
    "y_pred, y_true = query_model_predict(knn3, 'knn', datasets[2])\n",
    "show_model_metrics('knn', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets[0]\n",
    "\n",
    "# # dataset \n",
    "# # {'activation': 'relu', 'hidden_layer_data_sizes': 8} for dataset 0\n",
    "# # {'activation': 'tanh', 'hidden_layer_data_sizes': 10} for dataset 1 \n",
    "# # {'activation': 'tanh', 'hidden_layer_data_sizes': 10} for dataset 2\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [i for i in range(1, 11)],\n",
    "#     'activation': ['identity', 'logistic', 'tanh', 'relu']\n",
    "# }\n",
    "\n",
    "# X, y = dataset[:,:-1], dataset[:,-1]\n",
    "# search_result(mlp, 'mlp', X, y, param_grid)\n",
    "\n",
    "mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=10)\n",
    "y_pred, y_true = query_model_predict(mlp, 'mlp', datasets[0])\n",
    "show_model_metrics('mlp', y_true, y_pred)\n",
    "\n",
    "mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=10)\n",
    "y_pred, y_true = query_model_predict(mlp, 'mlp', datasets[1])\n",
    "show_model_metrics('mlp', y_true, y_pred)\n",
    "\n",
    "mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=10)\n",
    "y_pred, y_true = query_model_predict(mlp, 'mlp', datasets[2])\n",
    "show_model_metrics('mlp', y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_switch = 90\n",
    "# period_size = 2 * num_switch + 4 \n",
    "period_size = 2 * 4 # number of fake switches * number of links\n",
    "data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "data_size = 10000000\n",
    "processor = DataProcess(data_file, data_size, ds_config)\n",
    "processor.preprocess()\n",
    "\n",
    "delay_map = processor.query_delay_map()\n",
    "dataset = processor.query_dataset()\n",
    "start_range = range(0, 2000)\n",
    "test_range = range(2000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log topoguard+ result\n",
    "for s in range(0, 5100, 100):\n",
    "    start_range = range(s, s+2000)\n",
    "    test_range = range(s+2000, s+3000)\n",
    "\n",
    "    topoguard_plus = TopoGuardPlus(delay_map, start_range, test_range, ['2:1', '4:1'])\n",
    "    y_pred, y_true = topoguard_plus.fit()\n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('topoguard+', y_true, y_pred)\n",
    "    with open('topoguard_log/topoguard_log_{}.txt'.format(num_switch), 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log trustopo result\n",
    "for s in range(0, 5100, 100):\n",
    "    start_range = range(s, s+2000)\n",
    "    test_range = range(s+2000, s+3000)\n",
    "\n",
    "    trust_topo = TrustTopo(delay_map, start_range, test_range, ['2:1', '4:1'])\n",
    "    y_pred, y_true = trust_topo.fit()\n",
    "\n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('trust_topo', y_true, y_pred)\n",
    "    with open('trusttopo_log/trusttopo_log_{}.txt'.format(num_switch), 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log linkguard result\n",
    "for s in range(0, 5100, 100):\n",
    "    train_range = range(s, s+2000)\n",
    "    test_range = range(s+2000, s+3000)\n",
    "\n",
    "    link_guard = LinkGuard(delay_map, train_range, test_range, ['2:1', '4:1'])\n",
    "    y_pred, y_true = link_guard.fit()\n",
    "    \n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('link_guard', y_true, y_pred)\n",
    "    with open('linkguard_log/linkguard_log_{}.txt'.format(num_switch), 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log rlv result\n",
    "\n",
    "for s in range(0, 5100, 100):\n",
    "    rf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=20)\n",
    "    y_pred, y_true = query_model_predict(rf, 'rf', dataset[s*period_size:(s+3000)*period_size])\n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('rf', y_true, y_pred)\n",
    "    with open('rf_log/rf_log_{}.txt'.format(num_switch), 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log rlv result\n",
    "for s in range(0, 5100, 100):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=10, activation='tanh')\n",
    "    y_pred, y_true = query_model_predict(mlp, 'mlp', dataset[s*period_size:(s+3000)*period_size])\n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('mlp', y_true, y_pred)\n",
    "    with open('mlp_log/mlp_log_{}.txt'.format(num_switch), 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
