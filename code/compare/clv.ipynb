{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, cohen_kappa_score, confusion_matrix \n",
    "from spot import *\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess:\n",
    "    INVALID_ITEM = -1\n",
    "    def __init__(self, data_file, data_size, config):\n",
    "        self.file = data_file\n",
    "        self.data_size = data_size\n",
    "        self.fake_eps = config['fake_eps']\n",
    "        self.min_delay = config['min_delay']\n",
    "        self.max_delay = config['max_delay']\n",
    "        self.max_bw = np.log(config['max_bw']*1024*1024)\n",
    "        self.cong_rate = config['cong_rate']\n",
    "        self.stat_size = config['stat_size']\n",
    "        self.avg_weight = config['avg_weight']\n",
    "        self.is_opt = config['is_opt']\n",
    "        self.link_delay_map = {}\n",
    "        self.path_delay_map = {}\n",
    "        self.link_loads_map = {}\n",
    "        self.valid_delay_map = {}\n",
    "        self.valid_loads_map = {}\n",
    "        self.delay_limit_map = {}\n",
    "        self.quality_map = {}\n",
    "        self.dataset = []\n",
    "        self.target_dpids = self._get_target_dpids()\n",
    "    \n",
    "    def preprocess(self):\n",
    "        with open(self.file, 'r+') as fp:\n",
    "            for _ in range(self.data_size):\n",
    "                line = fp.readline()\n",
    "                if not line:\n",
    "                    break \n",
    "                self._get_link_delay(line)\n",
    "                self._get_data_item(line)\n",
    "                self._get_link_quality(line)\n",
    "    \n",
    "    def _get_target_dpids(self):\n",
    "        target_dpids = set()\n",
    "        for ep in self.fake_eps:\n",
    "            dpid = ep[0]\n",
    "            target_dpids.add(dpid)\n",
    "        return target_dpids\n",
    "    \n",
    "    def _get_link_delay(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        loads = float(res_list[-1])\n",
    "        path_delay = float(res_list[-2])\n",
    "        cs_delay = float(res_list[-3])\n",
    "        sc_delay = float(res_list[-4])\n",
    "        link_delay = path_delay - (cs_delay + sc_delay)/2\n",
    "        if link_delay < self.min_delay:\n",
    "            link_delay = self.min_delay\n",
    "        link_delays = self.link_delay_map.setdefault(link, [])\n",
    "        link_delays.append(link_delay)\n",
    "        link_loads = self.link_loads_map.setdefault(link, [])\n",
    "        link_loads.append(loads)\n",
    "        path_delays = self.path_delay_map.setdefault(link, [])\n",
    "        path_delays.append(path_delay)\n",
    "    \n",
    "    def _get_valid_delay(self, link, delay):\n",
    "        valid_list = self.valid_delay_map.setdefault(link, [self.max_delay])\n",
    "        if delay == DataProcess.INVALID_ITEM:\n",
    "            if len(valid_list) < self.stat_size:\n",
    "                item_list = self.link_delay_map[link]\n",
    "                s_loc = -min(len(item_list), self.stat_size)\n",
    "                valid_delay = np.median(item_list[s_loc:])\n",
    "            else:\n",
    "                valid_delay = np.median(valid_list)\n",
    "        else:\n",
    "            if len(valid_list) >= self.stat_size:\n",
    "                valid_list.pop(0)\n",
    "            valid_list.append(delay)\n",
    "            if len(valid_list) < len(self.avg_weight):\n",
    "                valid_delay = sum(valid_list)/len(valid_list)\n",
    "            else:\n",
    "                s_loc = -len(self.avg_weight)\n",
    "                valid_delay = sum([self.avg_weight[i]*d \\\n",
    "                    for i, d in enumerate(valid_list[s_loc:])])\n",
    "        return valid_delay\n",
    "        \n",
    "    def _get_valid_loads(self, link, loads):\n",
    "        valid_list = self.valid_loads_map.setdefault(link, [0])\n",
    "        if loads == DataProcess.INVALID_ITEM:\n",
    "            valid_loads = 0\n",
    "        else:\n",
    "            if len(valid_list) >= self.stat_size:\n",
    "                valid_list.pop(0)\n",
    "            valid_list.append(loads)\n",
    "            if len(valid_list) < len(self.avg_weight):\n",
    "                valid_loads = sum(valid_list)/len(valid_list)\n",
    "            else:\n",
    "                s_loc = -len(self.avg_weight)\n",
    "                valid_loads = sum([self.avg_weight[i]*d \\\n",
    "                    for i, d in enumerate(valid_list[s_loc:])])\n",
    "        return valid_loads\n",
    "        \n",
    "    def _get_link_quality(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        delay = float(res_list[-2])\n",
    "        loads = float(res_list[-1])\n",
    "        \n",
    "        # The key idea of this tricks locates at that, the delay beyond a so called 'max_delay' should\n",
    "        # make no sense to indicate the link quality, as well as the reliability of the links.\n",
    "        # Statistic of the median delay of the links of the target dpid, so as to obtain the valid delay\n",
    "        # which is used to indicated the reliability of the links.\n",
    "        \n",
    "        if self.is_opt:\n",
    "            if delay > self.max_delay:\n",
    "                delay = self._get_valid_delay(link, DataProcess.INVALID_ITEM)\n",
    "                loads = self._get_valid_loads(link, DataProcess.INVALID_ITEM)\n",
    "            else:\n",
    "                delay = self._get_valid_delay(link, delay)\n",
    "                loads = self._get_valid_loads(link, loads)\n",
    "            \n",
    "        tq = 1 - np.tanh(delay/self.max_delay)\n",
    "        lq = 1 / (1 + np.exp(-(loads-self.max_bw*self.cong_rate)))\n",
    "        \n",
    "        # The degree of indicating link quality mainly depends on the delay quality, e.g., \n",
    "        # Link A: high loads, low delay\n",
    "        # Link B: low loads, low delay\n",
    "        # Link C: high loads, high delay\n",
    "        # Link D: low loads, high delay \n",
    "        # we hat A > B > C > D\n",
    "        # Eq. Q = (Qt + np.exp(Qt-max_Qt) * Ql) / 2, where max_Qt = 1-np.tanh(0) = 1\n",
    "        quality = tq * (1 + np.exp(tq - 1) * lq) / 2 \n",
    "        quality_list = self.quality_map.setdefault(link, [])\n",
    "        quality_list.append(quality)\n",
    "    \n",
    "    def _get_data_item(self, line):\n",
    "        res_list = line.split()\n",
    "        link = ''.join(res_list[:3])\n",
    "        dpid = link.split(':')[0]\n",
    "        if not dpid in self.target_dpids:\n",
    "            return \n",
    "        path_delay = float(res_list[-2])\n",
    "        cs_delay = float(res_list[-3])\n",
    "        sc_delay = float(res_list[-4])\n",
    "        delay = path_delay - (cs_delay + sc_delay)/2\n",
    "        if delay < self.min_delay:\n",
    "            delay = self.min_delay\n",
    "        if any([x in link for x in self.fake_eps]):\n",
    "            is_forged_link = True \n",
    "        else:\n",
    "            is_forged_link = False \n",
    "        data_item = [cs_delay, sc_delay, delay, is_forged_link]\n",
    "        self.dataset.append(data_item)\n",
    "    \n",
    "    def query_quality_map(self):\n",
    "        return self.quality_map \n",
    "    \n",
    "    def query_delay_map(self):\n",
    "        return self.link_delay_map\n",
    "    \n",
    "    def query_delay_map2(self):\n",
    "        return self.path_delay_map\n",
    "    \n",
    "    def query_dataset(self):\n",
    "        return np.array(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathSimulator:\n",
    "    def __init__(self, quality_map, gamma):\n",
    "        self.quality_map = quality_map\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def simulator(self, corr_links, simu_range):\n",
    "        factors = {link:1 for link in corr_links}\n",
    "        records = []\n",
    "        for i in simu_range:\n",
    "            max_quality = 0 \n",
    "            for link in corr_links:\n",
    "                q = self.quality_map[link][i] * factors[link]\n",
    "                if q >= max_quality:\n",
    "                    max_quality = q\n",
    "                    best_link = link \n",
    "            records.append(corr_links.index(best_link))\n",
    "            # update discount factor\n",
    "            for link in corr_links:\n",
    "                if link == best_link:\n",
    "                    factors[link] *= self.gamma\n",
    "                else:\n",
    "                    factors[link] = 1\n",
    "        return records\n",
    "    \n",
    "    def calc_period_list(self, records):\n",
    "        idx = 0\n",
    "        period_list = [idx]\n",
    "        while idx < len(records):\n",
    "            curr_rec = records[idx]\n",
    "            while idx < len(records) and curr_rec == records[idx]:\n",
    "                idx += 1\n",
    "            if idx < len(records):\n",
    "                curr_rec = records[idx]\n",
    "                while idx < len(records) and curr_rec == records[idx]:\n",
    "                    idx += 1\n",
    "            period_list.append(idx)\n",
    "        return period_list\n",
    "            \n",
    "    def calc_k_map(self, corr_links, records):\n",
    "        k_map = {link: [] for link in corr_links}\n",
    "        curr_rec = records[0]\n",
    "        counter = 0\n",
    "        for rec in records:\n",
    "            if rec == curr_rec:\n",
    "                counter += 1\n",
    "            else:\n",
    "                curr_link = corr_links[curr_rec]\n",
    "                k_map[curr_link].append(counter)\n",
    "                curr_rec = rec\n",
    "                counter = 1\n",
    "        return k_map            \n",
    "\n",
    "    def calc_entropy_list(self, records, interval):\n",
    "        period_list = self.calc_period_list(records)\n",
    "        period_length = len(period_list)\n",
    "        entropy_list = []\n",
    "        num_1st = 0\n",
    "        idx = 0\n",
    "        for idx in range(0, period_length-interval-1):\n",
    "            start = period_list[idx]\n",
    "            end_idx = min(idx+interval, period_length-1)\n",
    "            end = period_list[end_idx]\n",
    "            seqs = np.array(records[start:end])\n",
    "            num_1st = np.sum(seqs == 1)\n",
    "            prob = num_1st / len(seqs)\n",
    "            entropy = -prob * np.log2(prob) - (1-prob) * np.log2(1-prob)\n",
    "            entropy_list.append(entropy)\n",
    "        return entropy_list\n",
    "    \n",
    "    # discrete\n",
    "    def get_k_dist(self, k_map, link):\n",
    "        assert(link in k_map)\n",
    "        k_values = k_map[link]\n",
    "        k_set = set(k_values)\n",
    "        k_prob = [k_values.count(k)/len(k_values) for k in k_set]\n",
    "        return list(k_set), k_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CorrLinksVerification:\n",
    "    def __init__(self, quality_map, train_corr_links, test_corr_links, is_fake, run_config):\n",
    "        self.quality_map = quality_map\n",
    "        self.train_corr_links = train_corr_links\n",
    "        self.test_corr_links = test_corr_links\n",
    "        self.is_fake = is_fake\n",
    "        self.interval = run_config['interval']  \n",
    "        self.q_e = run_config['q_e']\n",
    "        self.q_k = run_config['q_k']\n",
    "        self.depth_e = run_config['depth_e']\n",
    "        self.depth_k = run_config['depth_k']\n",
    "        self.simu = MultiPathSimulator(quality_map, run_config['gamma'])\n",
    "    \n",
    "    def get_refer_link(self):\n",
    "        train_link_set = set(self.train_corr_links)\n",
    "        test_link_set = set(self.test_corr_links)\n",
    "        refer_link = train_link_set & test_link_set\n",
    "        return refer_link.pop()\n",
    "                        \n",
    "    def calc_e_thr(self, train_list_init, train_list_supl):\n",
    "        rev_list_init = [-x for x in train_list_init]\n",
    "        rev_list_supl = [-x for x in train_list_supl]\n",
    "        s = dSPOT(self.q_e, depth=self.depth_e)\n",
    "        s.fit(rev_list_init, rev_list_supl)\n",
    "        s.initialize(verbose=False)\n",
    "        thr = s.run()\n",
    "        e_thr_list = [-x for x in thr['thresholds']]\n",
    "        return e_thr_list\n",
    "    \n",
    "    def _calc_k_thr_base(self, k_list):\n",
    "        k_set = set(k_list)\n",
    "        probs = [k_list.count(k)/len(k_list) for k in k_set]\n",
    "        k_exp = sum([probs[i]*k for i,k in enumerate(k_set)])\n",
    "        p_hat = 1 / k_exp\n",
    "        cum_dist = p_hat\n",
    "        idx = 0\n",
    "        while cum_dist < 1 - self.q_k:\n",
    "            cum_dist += (p_hat * ((1-p_hat) ** (idx-1)))\n",
    "            idx += 1\n",
    "        k_thr = max(idx, max(k_set)) + 1\n",
    "        return k_thr\n",
    "    \n",
    "    def calc_k_thr(self, k_list_init, k_list_supl):\n",
    "        k_thr_list = []\n",
    "        k_list = k_list_init[:self.depth_k]\n",
    "        k_thr0 = self._calc_k_thr_base(k_list_init)\n",
    "        for item in k_list_supl:\n",
    "            k_list.pop(0)\n",
    "            k_list.append(item)\n",
    "            k_thr = self._calc_k_thr_base(k_list)\n",
    "            k_thr = max(k_thr, k_thr0)\n",
    "            k_thr_list.append(k_thr)\n",
    "        return k_thr_list\n",
    "    \n",
    "    def train(self, train_range, test_range):\n",
    "        train_records_init = self.simu.simulator(self.train_corr_links, train_range)\n",
    "        train_records_supl = self.simu.simulator(self.train_corr_links, test_range)\n",
    "        \n",
    "        train_e_list_init = self.simu.calc_entropy_list(train_records_init, self.interval)\n",
    "        train_e_list_supl = self.simu.calc_entropy_list(train_records_supl, self.interval)\n",
    "        e_thr_list = self.calc_e_thr(train_e_list_init, train_e_list_supl)\n",
    "        \n",
    "        train_k_map_init = self.simu.calc_k_map(self.train_corr_links, train_records_init)\n",
    "        train_k_map_supl = self.simu.calc_k_map(self.train_corr_links, train_records_supl)\n",
    "        refer_link = self.get_refer_link()\n",
    "        train_k_list_init = train_k_map_init[refer_link]\n",
    "        train_k_list_supl = train_k_map_supl[refer_link]\n",
    "        k_thr_list = self.calc_k_thr(train_k_list_init, train_k_list_supl)\n",
    "        \n",
    "        test_records = self.simu.simulator(self.test_corr_links, test_range)\n",
    "        test_e_list = self.simu.calc_entropy_list(test_records, self.interval)\n",
    "        test_k_map = self.simu.calc_k_map(self.test_corr_links, test_records)\n",
    "        refer_link = self.get_refer_link()\n",
    "        test_k_list = test_k_map[refer_link]\n",
    "        \n",
    "        return test_records, train_records_supl, \\\n",
    "            e_thr_list, k_thr_list, test_e_list, test_k_list\n",
    "        \n",
    "    \n",
    "    def pred(self, test_records, supl_records, e_thr_list, k_thr_list, test_e_list, test_k_list):\n",
    "        y_pred = [0] * len(test_records)\n",
    "        supl_period_list = self.simu.calc_period_list(supl_records)\n",
    "        test_period_list = self.simu.calc_period_list(test_records)\n",
    "        \n",
    "        for i, curr_e in enumerate(test_e_list):\n",
    "            supl_loc = bisect.bisect(supl_period_list, test_period_list[i])\n",
    "            if supl_loc == 0:\n",
    "                e_thr = e_thr_list[0]\n",
    "            elif supl_loc >= len(e_thr_list)-1:\n",
    "                e_thr = e_thr_list[-1]\n",
    "            else:\n",
    "                s_loc = max(0, supl_loc-self.interval+1)\n",
    "                e_loc = min(supl_loc+self.interval, len(e_thr_list)-1)\n",
    "                e_thr = min(e_thr_list[s_loc:e_loc])\n",
    "            s_idx = test_period_list[i]\n",
    "            e_idx = test_period_list[i+self.interval-1]\n",
    "            if curr_e <= e_thr:\n",
    "                if not self.is_fake:\n",
    "                    print('[+] FP: curr_e: {} e_thr: {}'.format(curr_e, e_thr))\n",
    "                for j in range(s_idx, e_idx):\n",
    "                    y_pred[j] = 1\n",
    "        # print('test_e_list {}: {}\\n e_thr_list {}: {}\\n'.format(\n",
    "        #    len(test_e_list), test_e_list, len(e_thr_list), e_thr_list))\n",
    "        for i, curr_k in enumerate(test_k_list):\n",
    "            supl_loc = bisect.bisect(supl_period_list, test_period_list[i])\n",
    "            if supl_loc == 0:\n",
    "                k_thr = k_thr_list[0]\n",
    "            elif supl_loc >= len(k_thr_list)-1:\n",
    "                k_thr = k_thr_list[-1]\n",
    "            else:\n",
    "                k_thr = k_thr_list[supl_loc-1]\n",
    "            s_idx = test_period_list[i]\n",
    "            e_idx = test_period_list[i+1]\n",
    "            if curr_k > k_thr:\n",
    "                if not self.is_fake:\n",
    "                    print('[+] FP: curr_k: {} k_thr: {}'.format(curr_k, k_thr))\n",
    "                for j in range(s_idx, e_idx):\n",
    "                    y_pred[j] = 1\n",
    "        # print('test_k_list {}: {}\\n k_thr_list {}: {}\\n'.format(\n",
    "        #    len(test_k_list), test_k_list, len(k_thr_list), k_thr_list))\n",
    "        return y_pred\n",
    "                \n",
    "    def fit(self, train_range, test_range):\n",
    "        test_records, supl_records, e_thr_list, k_thr_list, \\\n",
    "            test_e_list, test_k_list = self.train(train_range, test_range)\n",
    "        y_pred = self.pred(test_records, supl_records, \\\n",
    "            e_thr_list, k_thr_list, test_e_list, test_k_list)\n",
    "        y_true = [self.is_fake] * len(test_records)\n",
    "        \n",
    "        return y_pred, y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_metrics(model_name, y_true, y_pred):   \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cnf_matrix.ravel()\n",
    "    \n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    \n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "    \n",
    "    Pr = precision_score(y_true, y_pred)\n",
    "    F1 = f1_score(y_true, y_pred)\n",
    "    AUC = roc_auc_score(y_true, y_pred)\n",
    "    print(f'''\n",
    "          name : {model_name},\n",
    "          TPR: {TPR},\n",
    "          FPR: {FPR},\n",
    "          Precision: {Pr},\n",
    "          f1_score: {F1},\n",
    "          auc_score: {AUC}\n",
    "          ''')\n",
    "    return TPR, FPR, Pr, F1, AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dataset parameters and running parameters \n",
    "\n",
    "# data_size: the number of data items\n",
    "# fake_eps: the endpoints of the fabricated links.\n",
    "# min_delay: tx delay, set to 0.5ms in Mininet.\n",
    "# max_delay: tx delay, set to 5ms empirically, a delay larger than max_delay should no longer \n",
    "#   indicate the link quality effectively.\n",
    "# max_bw: maximum bandwidth, set to 100Mb/s in Mininet.\n",
    "# cong_rate: congestion ratio, 80%.\n",
    "# stat_size: do the statistic each 100 items.\n",
    "# avg_weight: moving average of the latest 4 items.\n",
    "\n",
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "    'is_opt': False\n",
    "}\n",
    "\n",
    "# q is risk probability, the smaller, the safer.\n",
    "# depth is update window, e.g., for entropy, updating each 3 items.\n",
    "# gamma: discount ratio.\n",
    "# interval: statistic periods, e.g., do the statistic each 10 periods. \n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 5,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "}\n",
    "\n",
    "normal_link_1_1 = '2:2->1:2'\n",
    "normal_link_1_2 = '2:3->3:2'\n",
    "normal_link_1_3 = '2:4->102:1'\n",
    "fabricated_link_1 = '2:1->4:1'\n",
    "\n",
    "normal_link_2_1 = '4:2->3:3'\n",
    "normal_link_2_2 = '4:3->5:2'\n",
    "normal_link_2_3 = '4:4->104:1'\n",
    "fabricated_link_2 = '4:1->2:1'\n",
    "\n",
    "train_corr_links_1 = [normal_link_1_1, normal_link_1_2]\n",
    "test_corr_links_1_1 = [normal_link_1_1, normal_link_1_3]\n",
    "test_corr_links_1_2 = [normal_link_1_1, fabricated_link_1]\n",
    "\n",
    "train_corr_links_2 = [normal_link_2_1, normal_link_2_2]\n",
    "test_corr_links_2_1 = [normal_link_2_1, normal_link_2_3]\n",
    "test_corr_links_2_2 = [normal_link_2_1, fabricated_link_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [00:00<00:00, 4351.88it/s]\n",
      "100%|██████████| 449/449 [00:00<00:00, 4437.73it/s]\n",
      "100%|██████████| 427/427 [00:00<00:00, 27620.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] FP: curr_k: 4 k_thr: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:00<00:00, 26775.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          name : clv,\n",
      "          TPR: 0.997,\n",
      "          FPR: 0.0025,\n",
      "          Precision: 0.9974987493746873,\n",
      "          f1_score: 0.997249312328082,\n",
      "          auc_score: 0.99725\n",
      "          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:00<00:00, 14950.99it/s]\n",
      "100%|██████████| 447/447 [00:00<00:00, 14929.68it/s]\n",
      "100%|██████████| 464/464 [00:00<00:00, 8312.68it/s]\n",
      "100%|██████████| 464/464 [00:00<00:00, 8695.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          name : clv,\n",
      "          TPR: 0.993,\n",
      "          FPR: 0.0,\n",
      "          Precision: 1.0,\n",
      "          f1_score: 0.9964877069744105,\n",
      "          auc_score: 0.9964999999999999\n",
      "          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:00<00:00, 17683.55it/s]\n",
      "100%|██████████| 423/423 [00:00<00:00, 16330.91it/s]\n",
      "100%|██████████| 279/279 [00:00<00:00, 4784.22it/s]\n",
      "100%|██████████| 279/279 [00:00<00:00, 4911.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          name : clv,\n",
      "          TPR: 0.9975,\n",
      "          FPR: 0.0,\n",
      "          Precision: 1.0,\n",
      "          f1_score: 0.9987484355444306,\n",
      "          auc_score: 0.99875\n",
      "          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# basic test\n",
    "num_switch = 5\n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 3,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "}\n",
    "\n",
    "data_files = [\n",
    "    r'../dataset/{}switches/pcap_link_0.5_no_load_no_attack.txt'.format(num_switch),\n",
    "    #r'../dataset/{}switches/pcap_link_0.5_load_no_attack.txt'.format(num_switch), \n",
    "    r'../dataset/{}switches/pcap_link_0.5_no_load_attack.txt'.format(num_switch),\n",
    "    r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch),\n",
    "    ]\n",
    "\n",
    "data_size = 14*3000\n",
    "\n",
    "processors = [\n",
    "    DataProcess(data_file, data_size, ds_config) for data_file in data_files]\n",
    "\n",
    "for p in processors:\n",
    "    p.preprocess()\n",
    "\n",
    "for p in processors:\n",
    "    quality_map = p.query_quality_map()\n",
    "    train_range = range(0, 2000)\n",
    "    test_range = range(2000, 3000)\n",
    "\n",
    "    clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "    y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "    clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "    y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "    clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "    y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "    clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "    y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "    y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "    y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "\n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dataset parameters and running parameters \n",
    "\n",
    "# data_size: the number of data items\n",
    "# fake_eps: the endpoints of the fabricated links.\n",
    "# min_delay: tx delay, set to 0.5ms in Mininet.\n",
    "# max_delay: tx delay, set to 5ms empirically, a delay larger than max_delay should no longer \n",
    "#   indicate the link quality effectively.\n",
    "# max_bw: maximum bandwidth, set to 100Mb/s in Mininet.\n",
    "# cong_rate: congestion ratio, 80%.\n",
    "# stat_size: do the statistic each 100 items.\n",
    "# avg_weight: moving average of the latest 4 items.\n",
    "\n",
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "# q is risk probability, the smaller, the safer.\n",
    "# depth is update window, e.g., for entropy, updating each 3 items.\n",
    "# gamma: discount ratio.\n",
    "# interval: statistic periods, e.g., do the statistic each 10 periods. \n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 3,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "}\n",
    "\n",
    "normal_link_1_1 = '2:2->1:2'\n",
    "normal_link_1_2 = '2:3->3:2'\n",
    "normal_link_1_3 = '2:4->102:1'\n",
    "fabricated_link_1 = '2:1->4:1'\n",
    "\n",
    "normal_link_2_1 = '4:2->3:3'\n",
    "normal_link_2_2 = '4:3->5:2'\n",
    "normal_link_2_3 = '4:4->104:1'\n",
    "fabricated_link_2 = '4:1->2:1'\n",
    "\n",
    "train_corr_links_1 = [normal_link_1_1, normal_link_1_2]\n",
    "test_corr_links_1_1 = [normal_link_1_1, normal_link_1_3]\n",
    "test_corr_links_1_2 = [normal_link_1_1, fabricated_link_1]\n",
    "\n",
    "train_corr_links_2 = [normal_link_2_1, normal_link_2_2]\n",
    "test_corr_links_2_1 = [normal_link_2_1, normal_link_2_3]\n",
    "test_corr_links_2_2 = [normal_link_2_1, fabricated_link_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot of the quality of the corr-links\n",
    "sns.lineplot(data={\n",
    "    'normal_link_1_1': quality_map[normal_link_1_1][1500:1600],\n",
    "    'normal_link_1_2': quality_map[normal_link_1_2][1500:1600],\n",
    "    'normal_link_1_3': quality_map[normal_link_1_3][1500:1600],\n",
    "    'fabricated_link_1': quality_map[fabricated_link_1][1500:1600]\n",
    "})\n",
    "\n",
    "# check size\n",
    "print(len(quality_map[normal_link_1_1]), len(quality_map[normal_link_1_2]), \\\n",
    "    len(quality_map[normal_link_1_3]), len(quality_map[fabricated_link_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_switch = 10 # range from 10 to 100\n",
    "data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "data_size = 10000000\n",
    "processor = DataProcess(data_file, data_size, ds_config)\n",
    "processor.preprocess()\n",
    "quality_map = processor.query_quality_map()\n",
    "        \n",
    "# log clv result\n",
    "for s in range(0, 5100, 100):\n",
    "    train_range = range(s, s+2000)\n",
    "    test_range = range(s+2000, s+3000)\n",
    "\n",
    "    clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "    y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "    clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "    y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "    clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "    # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "    y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "    clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "    y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "    y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "    y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "    \n",
    "    TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "    if processor.is_opt: \n",
    "        log_file = 'clv_log_opt/clv_log_opt_{}.txt'.format(num_switch)\n",
    "    else:\n",
    "        log_file = 'clv_log_raw/clv_log_raw_{}.txt'.format(num_switch)\n",
    "    with open(log_file, 'a+') as fp:\n",
    "        fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "            TPR, FPR, Pr, F1, AUC\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Compare performance with different parameters   #\n",
    "#=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying q value\n",
    "\n",
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "    'is_opt': True,\n",
    "    }\n",
    "        \n",
    "for num_switch in range(10, 110, 10):\n",
    "    data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "    data_size = 10000000\n",
    "    processor = DataProcess(data_file, data_size, ds_config)\n",
    "    processor.preprocess()\n",
    "\n",
    "    delay_map2 = processor.query_delay_map2()\n",
    "    quality_map = processor.query_quality_map()\n",
    "\n",
    "    for q in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "        run_config = {\n",
    "            'q_e': q,\n",
    "            'q_k': q,\n",
    "            'depth_e': 3,\n",
    "            'depth_k': 100,\n",
    "            'gamma': 0.9,\n",
    "            'interval':10,\n",
    "            }\n",
    "            # log clv result\n",
    "        for s in range(0, 5100, 100):\n",
    "            train_range = range(s, s+2000)\n",
    "            test_range = range(s+2000, s+3000)\n",
    "\n",
    "            clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "            y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "            clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "            y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "            clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "            # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "            y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "            clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "            y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "            y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "            y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "            \n",
    "            TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "            with open('clv_log_1/q_{:.0e}/clv_log_{}.txt'.format(q, num_switch), 'a+') as fp:\n",
    "                fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                    TPR, FPR, Pr, F1, AUC\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying gamma value\n",
    "\n",
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "    'is_opt': True,\n",
    "    }\n",
    "        \n",
    "for num_switch in range(10, 110, 10):\n",
    "    data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "    data_size = 10000000\n",
    "    processor = DataProcess(data_file, data_size, ds_config)\n",
    "    processor.preprocess()\n",
    "\n",
    "    delay_map2 = processor.query_delay_map2()\n",
    "    quality_map = processor.query_quality_map()\n",
    "\n",
    "    for gamma in [0.8, 0.85, 0.9, 0.95]:\n",
    "        run_config = {\n",
    "            'q_e': 1e-5,\n",
    "            'q_k': 1e-5,\n",
    "            'depth_e': 3,\n",
    "            'depth_k': 100,\n",
    "            'gamma': gamma,\n",
    "            'interval':10,\n",
    "            }\n",
    "            # log clv result\n",
    "        for s in range(0, 5100, 100):\n",
    "            train_range = range(s, s+2000)\n",
    "            test_range = range(s+2000, s+3000)\n",
    "\n",
    "            clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "            y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "            clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "            y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "            clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "            # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "            y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "            clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "            y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "            y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "            y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "            \n",
    "            TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "            with open('clv_log_2/gamma_{}/clv_log_{}.txt'.format(gamma, num_switch), 'a+') as fp:\n",
    "                fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                    TPR, FPR, Pr, F1, AUC\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying delay_limit value\n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 3,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "}\n",
    "\n",
    "for num_switch in range(10, 110, 10):\n",
    "    for max_delay in [4, 5, 6, 7]:\n",
    "        ds_config = {\n",
    "            'fake_eps': ['2:1', '4:1'],\n",
    "            'min_delay': 0.5,\n",
    "            'max_delay': max_delay,\n",
    "            'max_bw': 100,\n",
    "            'cong_rate': 0.8,\n",
    "            'stat_size': 100,\n",
    "            'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "            'is_opt': True,\n",
    "        }\n",
    "        \n",
    "        data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "        data_size = 10000000\n",
    "        processor = DataProcess(data_file, data_size, ds_config)\n",
    "        processor.preprocess()\n",
    "        quality_map = processor.query_quality_map()\n",
    "\n",
    "        for s in range(0, 5100, 100):\n",
    "            train_range = range(s, s+2000)\n",
    "            test_range = range(s+2000, s+3000)\n",
    "\n",
    "            clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "            y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "            clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "            y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "            clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "            # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "            y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "            clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "            y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "            y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "            y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "            \n",
    "            TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "            with open('clv_log_3/limit_{}/clv_log_{}.txt'.format(max_delay, num_switch), 'a+') as fp:\n",
    "                fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                    TPR, FPR, Pr, F1, AUC\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying interval value\n",
    "\n",
    "ds_config = {\n",
    "    'fake_eps': ['2:1', '4:1'],\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 5,\n",
    "    'max_bw': 100,\n",
    "    'cong_rate': 0.8,\n",
    "    'stat_size': 100,\n",
    "    'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "    'is_opt': True,\n",
    "    }\n",
    "\n",
    "for num_switch in range(10, 110, 10):\n",
    "    for interval in [5, 10, 15, 20]:\n",
    "        run_config = {\n",
    "            'q_e': 1e-5,\n",
    "            'q_k': 1e-5,\n",
    "            'depth_e': 3,\n",
    "            'depth_k': 100,\n",
    "            'gamma': 0.9,\n",
    "            'interval':interval,\n",
    "            }\n",
    "        \n",
    "        data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "        data_size = 10000000\n",
    "        processor = DataProcess(data_file, data_size, ds_config)\n",
    "        processor.preprocess()\n",
    "        quality_map = processor.query_quality_map()\n",
    "\n",
    "        for s in range(0, 5100, 100):\n",
    "            train_range = range(s, s+2000)\n",
    "            test_range = range(s+2000, s+3000)\n",
    "\n",
    "            clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "            y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "            clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "            y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "            clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "            # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "            y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "            clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "            y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "            y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "            y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "            \n",
    "            TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "            with open('clv_log_4/interval_{}/clv_log_{}.txt'.format(interval,num_switch), 'a+') as fp:\n",
    "                fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                    TPR, FPR, Pr, F1, AUC\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying fake links\n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 3,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "    }\n",
    "\n",
    "num_fake_links = [9]\n",
    "\n",
    "for n in num_fake_links:\n",
    "    fake_eps = ['{}:1'.format(3*i+2) for i in range(n)] + \\\n",
    "        ['{}:1'.format(3*i+4) for i in range(n)]\n",
    "    ds_config = {\n",
    "        'fake_eps': fake_eps,\n",
    "        'min_delay': 0.5,\n",
    "        'max_delay': 5,\n",
    "        'max_bw': 100,\n",
    "        'cong_rate': 0.8,\n",
    "        'stat_size': 100,\n",
    "        'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "        'is_opt': True,\n",
    "    }\n",
    "    data_file = r'../dataset/30_switches_{}_fake_links/pcap_link_0.5_load_attack.txt'.format(n)\n",
    "    data_size = 10000000\n",
    "    processor = DataProcess(data_file, data_size, ds_config)\n",
    "    processor.preprocess()\n",
    "\n",
    "    quality_map = processor.query_quality_map()\n",
    "\n",
    "    for s in range(0, 5100, 100):\n",
    "        train_range = range(s, s+2000)\n",
    "        test_range = range(s+2000, s+3000)\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i in range(n):\n",
    "            dpid = i * 3 + 2\n",
    "            if i == 0:\n",
    "                normal_link_1 = '{}:2->{}:2'.format(dpid, dpid-1)\n",
    "            else:\n",
    "                normal_link_1 = '{}:2->{}:3'.format(dpid, dpid-1)\n",
    "            normal_link_2 = '{}:3->{}:2'.format(dpid, dpid+1)\n",
    "            normal_link_3 = '{}:4->{}:1'.format(dpid, dpid+100)\n",
    "            fabricated_link = '{}:1->{}:1'.format(dpid, dpid+2)\n",
    "            \n",
    "            train_corr_links = [normal_link_1, normal_link_2]\n",
    "            test_corr_links_1 = [normal_link_1, normal_link_3]\n",
    "            test_corr_links_2 = [normal_link_1, fabricated_link]\n",
    "            clv_1_0 = CorrLinksVerification(quality_map, train_corr_links, test_corr_links_1, 0, run_config)\n",
    "            y_pred_0, y_true_0=clv_1_0.fit(train_range, test_range)\n",
    "            clv_1_1 = CorrLinksVerification(quality_map, train_corr_links, test_corr_links_2, 1, run_config)\n",
    "            y_pred_1, y_true_1=clv_1_1.fit(train_range, test_range)\n",
    "            y_pred.extend(y_pred_0)\n",
    "            y_pred.extend(y_pred_1)\n",
    "            y_true.extend(y_true_0)\n",
    "            y_true.extend(y_true_1)\n",
    "\n",
    "        TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "        if not processor.is_opt:\n",
    "            file = 'clv_log_5/{}_fake_links/clv_log_raw_30.txt'.format(n)\n",
    "        else:\n",
    "            file = 'clv_log_5/{}_fake_links/clv_log_opt_30.txt'.format(n)\n",
    "        with open(file, 'a+') as fp:\n",
    "            fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                TPR, FPR, Pr, F1, AUC\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance with varying tx delays\n",
    "\n",
    "run_config = {\n",
    "    'q_e': 1e-5,\n",
    "    'q_k': 1e-5,\n",
    "    'depth_e': 3,\n",
    "    'depth_k': 100,\n",
    "    'gamma': 0.9,\n",
    "    'interval':10,\n",
    "    }\n",
    "\n",
    "tx_delay_list = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "for tx in tx_delay_list:\n",
    "    ds_config = {\n",
    "        'fake_eps': ['2:1', '4:1'],\n",
    "        'min_delay': tx,\n",
    "        'max_delay': 5,\n",
    "        'max_bw': 100,\n",
    "        'cong_rate': 0.8,\n",
    "        'stat_size': 100,\n",
    "        'avg_weight': [0.1, 0.2, 0.3, 0.4],\n",
    "        'is_opt': False,\n",
    "    }\n",
    "\n",
    "    # data_file = r'../dataset/{}switches/pcap_link_0.5_load_attack.txt'.format(num_switch)\n",
    "    data_file = r'../dataset/30_switches_multi_delays/pcap_link_{}_load_attack.txt'.format(tx)\n",
    "    data_size = 10000000\n",
    "    processor = DataProcess(data_file, data_size, ds_config)\n",
    "    processor.preprocess()\n",
    "    \n",
    "    quality_map = processor.query_quality_map()\n",
    "    for s in range(0, 5100, 100):\n",
    "        train_range = range(s, s+2000)\n",
    "        test_range = range(s+2000, s+3000)\n",
    "\n",
    "        clv_1_0 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_1, 0, run_config)\n",
    "        y_pred_1_0, y_true_1_0=clv_1_0.fit(train_range, test_range)\n",
    "\n",
    "        clv_1_1 = CorrLinksVerification(quality_map, train_corr_links_1, test_corr_links_1_2, 1, run_config)\n",
    "        y_pred_1_1, y_true_1_1=clv_1_1.fit(train_range, test_range)\n",
    "\n",
    "        clv_2_0 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_1, 0, run_config)\n",
    "        # y_pred_0, y_true_0=clv0.fit(train_range, test_range)\n",
    "        y_pred_2_0, y_true_2_0=clv_2_0.fit(train_range, test_range) \n",
    "\n",
    "        clv_2_1 = CorrLinksVerification(quality_map, train_corr_links_2, test_corr_links_2_2, 1, run_config)\n",
    "        y_pred_2_1, y_true_2_1=clv_2_1.fit(train_range, test_range)\n",
    "\n",
    "        y_pred = y_pred_1_0 + y_pred_1_1 + y_pred_2_0 + y_pred_2_1\n",
    "        y_true = y_true_1_0 + y_true_1_1 + y_true_2_0 + y_true_2_1\n",
    "        \n",
    "        TPR, FPR, Pr, F1, AUC = show_model_metrics('clv', y_true, y_pred)\n",
    "        if not processor.is_opt:\n",
    "            file = 'clv_log_6/delay_{}ms/clv_log_raw_30.txt'.format(tx)\n",
    "        else:\n",
    "            file = 'clv_log_6/delay_{}ms/clv_log_opt_30.txt'.format(tx)\n",
    "        with open(file, 'a+') as fp:\n",
    "            fp.write('TPR:{},FPR:{},Pr:{},F1:{},AUC:{}\\n'.format(\n",
    "                TPR, FPR, Pr, F1, AUC\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
